{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from numpy import zeros, ones, expand_dims, asarray\n",
    "from numpy.random import randn, randint\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Concatenate\n",
    "from keras.layers import LeakyReLU, Dropout, Embedding\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras import initializers\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2 as cv\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from PIL import Image\n",
    "\n",
    "# define a function to generate random points in the latent space\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    z_input = np.random.uniform(-1, 1, size=(n_samples , latent_dim))\n",
    "    return z_input\n",
    "\n",
    "# define a function to generate fake samples with the generator model\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict(z_input)  \n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return images, y\n",
    "\n",
    "# define a function to generate real samples from the Fashion-MNIST dataset\n",
    "def generate_real_samples(X_train, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, X_train.shape[0], n_samples) \n",
    "    # retrieve selected images\n",
    "    X = X_train[ix]  \n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1)) \n",
    "    return X, y\n",
    "\n",
    "# define a function to summarize the performance of the GAN model\n",
    "def summarize_performance(step, g_model,num_data, latent_dim, n_samples=100,):\n",
    "    # generate fake samples\n",
    "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # rescale pixel values from [-1,1] to [0,1]\n",
    "    X = (X + 1) / 2.0\n",
    "    # save the generator model to a file\n",
    "    filename2 = 'model_data_batch_%04d_%04d_.h5' % (num_data,step+1)\n",
    "    g_model.save(filename2)\n",
    "    # copy the saved model to a directory\n",
    "    import shutil\n",
    "    shutil.copy(filename2,r'C:\\Users\\Doaa\\Desktop\\Final project 2023\\Models')\n",
    "    # print a message indicating that the model has been saved\n",
    "    print('>Saved: %s' % (filename2))\n",
    "\n",
    "# define a function to generate and save output images during training\n",
    "def generate_and_save_output(model,epoch,test_input,num_data):\n",
    "    # generate images from the model\n",
    "    predictions = model(test_input,training=False)\n",
    "    # create a plot of generated images\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    # plot images\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4,4,i+1)\n",
    "        plt.imshow((predictions[i]*127.5+127.5).numpy().astype(np.uint8),cmap='gray')\n",
    "        plt.axis('off')\n",
    "    # save the plot to a file\n",
    "    plt.savefig(f'results_baseline/image_at_epoch_{epoch, num_data}.png')\n",
    "    plt.show()\n",
    "\n",
    "# define a function to create a line plot of loss for the GAN model and save it to a file\n",
    "def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n",
    "    # create a subplot for the loss plot\n",
    "    pyplot.subplot(2, 1, 1)\n",
    "    # plot the loss of the discriminator on real samples\n",
    "    pyplot.plot(d1_hist, label='d-real')\n",
    "    # plot the loss of the discriminator on fake samples\n",
    "    pyplot.plot(d2_hist, label='d-fake')\n",
    "    # plot the loss of the generator\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    # add a legend to the plot\n",
    "    pyplot.legend()\n",
    "    # create a subplot for the discriminator accuracy plot\n",
    "    pyplot.subplot(2, 1, 2)\n",
    "    # plot the accuracy of the discriminator on real samples\n",
    "    pyplot.plot(a1_hist, label='acc-real')\n",
    "    # plot the accuracy of the discriminator on fake samples\n",
    "    pyplot.plot(a2_hist, label='acc-fake')\n",
    "    # add a legend to the plot\n",
    "    pyplot.legend()\n",
    "    # save therest of the code as an image file and show the plot\n",
    "    pyplot.savefig('loss_plot.png')\n",
    "    pyplot.show()\n",
    "\n",
    "# define the generator model\n",
    "def define_generator(latent_dim):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image generator input\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 256 * 7 * 7\n",
    "    gen = Dense(n_nodes, kernel_initializer=init)(in_lat) # pass the input through a fully connected layer\n",
    "    gen = LeakyReLU(alpha=0.2)(gen) # apply LeakyReLU activation function to the output of the fully connected layer\n",
    "    gen = Reshape((7, 7, 256))(gen) # reshape the output to a 7x7x256 feature map\n",
    "    # upsample to 14x14\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen) # apply transpose convolution to upsample the feature map to 14x14x128\n",
    "    gen = LeakyReLU(alpha=0.2)(gen) # apply LeakyReLU activation function to the output of the transpose convolution layer\n",
    "    # upsample to 28x28\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen) # apply transpose convolution to upsample the feature map to 28x28x128\n",
    "    gen = LeakyReLU(alpha=0.2)(gen) # apply LeakyReLU activation function to the output of the transpose convolution layer\n",
    "    # output\n",
    "    out_layer = Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init)(gen) # apply convolution to generate a 28x28x1 output image\n",
    "    # define the model\n",
    "    model = Model(in_lat, out_layer)\n",
    "    return model\n",
    "\n",
    "# define the discriminator model\n",
    "def define_discriminator(in_shape=(28,28,1), n_classes=10):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=in_shape)\n",
    "    # downsample to 14x14\n",
    "    dis = Conv2D(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(in_image) # apply convolution to downsample the input image to 14x14x64\n",
    "    dis = LeakyReLU(alpha=0.2)(dis) # apply LeakyReLU activation function to the output of the convolution layer\n",
    "    dis = Dropout(0.4)(dis) # apply dropout regularization to the output\n",
    "    # downsample to 7x7\n",
    "    dis = Conv2D(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(dis) # apply convolution to downsample the feature map to 7x7x64\n",
    "    dis = LeakyReLU(alpha=0.2)(dis) # apply LeakyReLU activation function to the output of the convolution layer\n",
    "    dis = Dropout(0.4)(dis) # apply dropout regularization to the output\n",
    "    # flatten feature maps\n",
    "    dis = Flatten()(dis) # flatten the feature map to a 1D array\n",
    "    # real/fake output\n",
    "    out_classifier = Dense(1, activation='sigmoid')(dis) # apply a fully connected layer with a sigmoid activation function to generate a binary classification output\n",
    "    # define the model\n",
    "    model = Model(in_image, out_classifier)\n",
    "    # compile the model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def custom_loss(alpha):\n",
    "    # alpha is a hyperparameter that determines the relative weight of the BCE loss compared to the MAE loss\n",
    "    def loss(y_true, y_pred):\n",
    "        mae_loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "        bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        return alpha * bce_loss + (1 - alpha) * mae_loss\n",
    "    return loss\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect the generator output to the discriminator input\n",
    "    gan_output = d_model(g_model.output)\n",
    "    # define the gan model as taking latent space vectors and outputting a classification\n",
    "    model = Model(g_model.input, gan_output)\n",
    "    # compile the model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=custom_loss(0.5), optimizer=opt)\n",
    "    return model\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "def train(g_model, d_model, gan_model, X_train, latent_dim, num_data, n_epochs=10, n_batch=64):\n",
    "    # calculate the number of batches per epoch\n",
    "    bat_per_epo = int(X_train.shape[0] / n_batch)\n",
    "    # calculate the number of training steps\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # initialize lists to hold history of discriminator and generator loss and accuracy\n",
    "    d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n",
    "    # loop through each epoch\n",
    "    for epoch in range(n_epochs):\n",
    "        # loop through each batch in the epoch\n",
    "        for i in range(bat_per_epo):\n",
    "            # generate real samples and labels\n",
    "            X_real, y_real = generate_real_samples(X_train, n_batch)\n",
    "            # train the discriminator on real samples\n",
    "            d_loss_r, d_acc_r = d_model.train_on_batch(X_real, y_real)\n",
    "            # generate fake samples and labels\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_batch)\n",
    "            # train the discriminator on fake samples\n",
    "            d_loss_f, d_acc_f = d_model.train_on_batch(X_fake, y_fake)\n",
    "            # generate latent points as input for the generator\n",
    "            z_input = generate_latent_points(latent_dim, n_batch)                 \n",
    "            # create labels for the generator\n",
    "            y_gan = ones((n_batch, 1)) \n",
    "            # train the generator (via the GAN model) with the discriminator weights fixed\n",
    "            g_loss, g_acc = gan_model.train_on_batch(z_input, y_gan)\n",
    "        # print the loss and accuracy values for the discriminator and generator\n",
    "        print('>%d, dr[%.3f,%.3f], df[%.3f,%.3f], g[%.3f,%.3f], epoch is %d, num of data is %d' % (i+1, d_loss_r,d_acc_r, d_loss_f,d_acc_f, g_loss,g_acc,epoch+1,num_data))\n",
    "        # append the loss and accuracy values to the history lists\n",
    "        d1_hist.append(d_loss_r)\n",
    "        d2_hist.append(d_loss_f)\n",
    "        g_hist.append(g_loss)\n",
    "        a1_hist.append(d_acc_r)\n",
    "        a2_hist.append(d_acc_f)\n",
    "        # every 5 epochs, summarize the generator performance, save the generator and discriminator models to disk, plot the history of the loss and accuracy values, and generate and save sample outputs\n",
    "        if (epoch+1)%5 == 0:\n",
    "            summarize_performance(epoch, g_model,num_data, latent_dim)\n",
    "            save(g_model, d_model, epoch+1, num_data)\n",
    "            plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist,num_data)\n",
    "        generate_and_save_output(g_model,epoch+1 ,seed,num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adbd888",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "seed = generate_latent_points(100,16)\n",
    "# create the discriminator\n",
    "d_model = define_discriminator()\n",
    "# create the generator\n",
    "g_model = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# train model\n",
    "train(g_model, d_model, gan_model, data_set, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8ab9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
